{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import torch\n",
    "import torch.optim\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoderLayer\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
    "from utils import WaveNet, AudioMELSpectogramDataset #, ContentLoss, StyleLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "# create a module to normalize input image so we can easily put it in a\n",
    "# ``nn.Sequential``\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        # .view the mean and std to make them [C x 1 x 1] so that they can\n",
    "        # directly work with image Tensor of shape [B x C x H x W].\n",
    "        # B is batch size. C is number of channels. H is height and W is width.\n",
    "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        # normalize ``img``\n",
    "        return (img - self.mean) / self.std\n",
    "\n",
    "def CalcContentLoss(gen_feat,orig_feat):\n",
    "    #calculating the content loss of each layer by calculating the MSE between the content and generated features and adding it to content loss\n",
    "    content_l=torch.mean((gen_feat-orig_feat)**2)\n",
    "    return content_l\n",
    "\n",
    "def CalcStyleLoss(gen,style):\n",
    "    #Calculating the gram matrix for the style and the generated image\n",
    "    channel,height,width=gen.shape\n",
    "    G = torch.mm(gen.view(channel,height*width),gen.view(channel,height*width).t())\n",
    "    # print('G', G.max(), G.min())\n",
    "    A = torch.mm(style.view(channel,height*width),style.view(channel,height*width).t())\n",
    "    # print('A', A.max(), A.min())     \n",
    "    #Calcultating the style loss of each layer by calculating the MSE between the gram matrix of the style image and the generated image and adding it to style loss\n",
    "    style_l=torch.mean((G-A)**2)\n",
    "    return style_l\n",
    "\n",
    "def containsInf(x, text):\n",
    "    if torch.isinf(x).any():\n",
    "        print(f\"La imagen {text} contiene valores infinitos (inf).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleAutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, shared_channels, out_channels, kernel_size, dilation, nhead=8, dropout=0.2, dim_feedforward=1024):\n",
    "        super(StyleAutoEncoder, self).__init__()\n",
    "        self.sharedEncoder = WaveNet(in_channels, shared_channels, kernel_size, dilation, dropout)\n",
    "        self.speechEncoder = WaveNet(shared_channels, out_channels, kernel_size, dilation, dropout)\n",
    "        self.styleEncoder = WaveNet(shared_channels, out_channels, kernel_size, dilation, dropout)\n",
    "        self.style_encoder_attention = TransformerEncoderLayer(\n",
    "            d_model=out_channels,\n",
    "            nhead=8,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation='relu'\n",
    "        )\n",
    "        self.speech_encoder_attention = TransformerEncoderLayer(\n",
    "            d_model=out_channels,\n",
    "            nhead=8,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation='relu'\n",
    "        )\n",
    "        self.decoder_attention = TransformerDecoderLayer(\n",
    "            d_model=out_channels*2,\n",
    "            nhead=8,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation='relu'\n",
    "        )\n",
    "        self.decoder = WaveNet(out_channels*2, in_channels, kernel_size, dilation, dropout)\n",
    "\n",
    "    def forward(self, speech, style):\n",
    "\n",
    "        # print('speech', speech.shape)\n",
    "        # print('style', style.shape)\n",
    "\n",
    "        style = self.sharedEncoder(style)\n",
    "        speech = self.sharedEncoder(speech) \n",
    "\n",
    "        style = self.styleEncoder(style)\n",
    "        speech = self.speechEncoder(speech)\n",
    "        \n",
    "        style = style.permute(2, 0, 1)  # Reordenamos las dimensiones para que sea compatible con la capa de atenci√≥n\n",
    "        style_encoder = self.style_encoder_attention(style)\n",
    "        style = style.permute(1, 2, 0)\n",
    "        style_encoder = speech.permute(0, 1, 2)\n",
    "\n",
    "        speech = speech.permute(2, 0, 1)\n",
    "        speech_encoder = self.speech_encoder_attention(speech)\n",
    "        speech = speech.permute(1, 2, 0)\n",
    "        speech_encoder = speech.permute(0, 1, 2)\n",
    "        \n",
    "        # print('speech', speech.shape)\n",
    "        # print('style', style.shape)\n",
    "        out = torch.cat((speech, style), dim=1)\n",
    "        # print('speech_encoder', speech_encoder.shape)\n",
    "        # print('style_encoder', style_encoder.shape)\n",
    "        out_encoder = torch.cat((speech_encoder, style_encoder), dim=1)\n",
    "        \n",
    "        out = out.permute(2, 0, 1) \n",
    "        out_encoder = out.permute(0, 1, 2) \n",
    "\n",
    "        # print('out', out.shape)\n",
    "        # print('out_encoder', out_encoder.shape)\n",
    "\n",
    "        x = self.decoder_attention(out, out_encoder)\n",
    "        x = x.permute(1, 2, 0) \n",
    "\n",
    "        # print('x', x.shape)\n",
    "\n",
    "        x = self.decoder(x)\n",
    "        # x = x.unsqueeze(1)\n",
    "        # print('x', x.shape)\n",
    "\n",
    "        return x #.squeeze(0)\n",
    " \n",
    "class MelSpecVCAutoencoderModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, in_channels, shared_channels, out_channels, kernel_size, dilation, device, sample_rate, dropout=0.2):\n",
    "        super(MelSpecVCAutoencoderModule, self).__init__()\n",
    "        self.encoder_generator = StyleAutoEncoder(in_channels, shared_channels, out_channels, kernel_size, dilation, dropout)\n",
    "        self.todevice = device\n",
    "        self.speech_weight = 1\n",
    "        self.style_weight = 100\n",
    "\n",
    "    def forward(self, speech, style):\n",
    "        y = self.encoder_generator(speech, style)\n",
    "        return y\n",
    "\n",
    "    def debug(self, loss_speech, loss_style, loss):\n",
    "        # print('loss_speech', loss_speech.item())\n",
    "        # print('loss_style', loss_style.item())\n",
    "        # print('shape', loss.shape)\n",
    "        # print('loss', loss.item())\n",
    "        pass\n",
    "\n",
    "    def apply_loss(self, batch):\n",
    "        speech, style = batch\n",
    "        if speech.ndim == 2: speech = speech.unsqueeze(1)\n",
    "        if style.ndim == 2: style = style.unsqueeze(1)\n",
    "        y = self(speech, style)\n",
    "        loss_speech = CalcContentLoss(y, speech) * self.speech_weight\n",
    "        loss_style = CalcStyleLoss(y, style) * self.style_weight\n",
    "        containsInf(style, 'style')\n",
    "        containsInf(loss_style, 'loss_style')\n",
    "        containsInf(loss_speech, 'loss_speech')\n",
    "        containsInf(y, 'y')\n",
    "        print(y.shape, style.shape, speech.shape)\n",
    "        loss = loss_speech + loss_style\n",
    "        containsInf(loss, 'loss')\n",
    "        self.debug(loss_speech, loss_style, loss)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.apply_loss(batch)\n",
    "        self.log('train_loss', loss.item())\n",
    "        return loss \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.apply_loss(batch)\n",
    "        self.log('val_loss', loss.item())\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-2)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_loss'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\David Arcos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchaudio\\functional\\functional.py:571: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "sample_rate = 16000\n",
    "div_ratio = 0.8\n",
    "n_fft = 800\n",
    "audio_length = 3\n",
    "in_channels, shared_channels, out_channels, kernel_size, dilation = 128, 64, 32, 3, 12\n",
    "metadata = pd.read_csv('metadata.csv')\n",
    "dataset = AudioMELSpectogramDataset(metadata, device, sample_rate=sample_rate, audio_length=audio_length, n_fft=n_fft)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [int(len(dataset) * div_ratio), len(dataset) - int(len(dataset) * div_ratio)])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)\n",
    "model = MelSpecVCAutoencoderModule(in_channels, shared_channels, out_channels, kernel_size, dilation, device=device, sample_rate=sample_rate)\n",
    "model = model.to(device)\n",
    "model = model.to(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit None Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath='./checkpoints',\n",
    "    filename='{epoch}-{val_loss:.2f}-{val_r2:.2f}',\n",
    "    save_top_k=1,\n",
    "    monitor='val_loss',\n",
    "    every_n_epochs =1,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    max_epochs=10,\n",
    "    benchmark=True,\n",
    "    # deterministic=True,\n",
    "    precision=16,\n",
    "    accumulate_grad_batches=6,\n",
    "    callbacks=[\n",
    "        checkpoint_callback,\n",
    "        pl.callbacks.LearningRateMonitor(logging_interval='step')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type             | Params\n",
      "-------------------------------------------------------\n",
      "0 | encoder_generator | StyleAutoEncoder | 2.3 M \n",
      "-------------------------------------------------------\n",
      "2.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.3 M     Total params\n",
      "4.506     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013000011444091797,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7755644de44b149a96151dbdd5d10f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\David Arcos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speech torch.Size([16, 128, 241])\n",
      "style torch.Size([16, 128, 241])\n",
      "speech torch.Size([16, 32, 241])\n",
      "style torch.Size([16, 32, 241])\n",
      "speech_encoder torch.Size([16, 32, 241])\n",
      "style_encoder torch.Size([16, 32, 241])\n",
      "torch.Size([16, 128, 241]) torch.Size([16, 128, 241]) torch.Size([16, 128, 241])\n",
      "speech torch.Size([16, 128, 241])\n",
      "style torch.Size([16, 128, 241])\n",
      "speech torch.Size([16, 32, 241])\n",
      "style torch.Size([16, 32, 241])\n",
      "speech_encoder torch.Size([16, 32, 241])\n",
      "style_encoder torch.Size([16, 32, 241])\n",
      "torch.Size([16, 128, 241]) torch.Size([16, 128, 241]) torch.Size([16, 128, 241])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\David Arcos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012996673583984375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb92316dd494dc49700cc67be5d5136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speech torch.Size([16, 128, 241])\n",
      "style torch.Size([16, 128, 241])\n",
      "speech torch.Size([16, 32, 241])\n",
      "style torch.Size([16, 32, 241])\n",
      "speech_encoder torch.Size([16, 32, 241])\n",
      "style_encoder torch.Size([16, 32, 241])\n",
      "torch.Size([16, 128, 241]) torch.Size([16, 128, 241]) torch.Size([16, 128, 241])\n",
      "speech torch.Size([16, 128, 241])\n",
      "style torch.Size([16, 128, 241])\n",
      "speech torch.Size([16, 32, 241])\n",
      "style torch.Size([16, 32, 241])\n",
      "speech_encoder torch.Size([16, 32, 241])\n",
      "style_encoder torch.Size([16, 32, 241])\n",
      "torch.Size([16, 128, 241]) torch.Size([16, 128, 241]) torch.Size([16, 128, 241])\n",
      "speech torch.Size([16, 128, 241])\n",
      "style torch.Size([16, 128, 241])\n",
      "speech torch.Size([16, 32, 241])\n",
      "style torch.Size([16, 32, 241])\n",
      "speech_encoder torch.Size([16, 32, 241])\n",
      "style_encoder torch.Size([16, 32, 241])\n",
      "torch.Size([16, 128, 241]) torch.Size([16, 128, 241]) torch.Size([16, 128, 241])\n",
      "speech torch.Size([16, 128, 241])\n",
      "style torch.Size([16, 128, 241])\n",
      "speech torch.Size([16, 32, 241])\n",
      "style torch.Size([16, 32, 241])\n",
      "speech_encoder torch.Size([16, 32, 241])\n",
      "style_encoder torch.Size([16, 32, 241])\n",
      "torch.Size([16, 128, 241]) torch.Size([16, 128, 241]) torch.Size([16, 128, 241])\n",
      "speech torch.Size([16, 128, 241])\n",
      "style torch.Size([16, 128, 241])\n",
      "speech torch.Size([16, 32, 241])\n",
      "style torch.Size([16, 32, 241])\n",
      "speech_encoder torch.Size([16, 32, 241])\n",
      "style_encoder torch.Size([16, 32, 241])\n",
      "torch.Size([16, 128, 241]) torch.Size([16, 128, 241]) torch.Size([16, 128, 241])\n",
      "speech torch.Size([16, 128, 241])\n",
      "style torch.Size([16, 128, 241])\n",
      "speech torch.Size([16, 32, 241])\n",
      "style torch.Size([16, 32, 241])\n",
      "speech_encoder torch.Size([16, 32, 241])\n",
      "style_encoder torch.Size([16, 32, 241])\n",
      "torch.Size([16, 128, 241]) torch.Size([16, 128, 241]) torch.Size([16, 128, 241])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\David Arcos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_speech = train_dataset[120]\n",
    "mel_spectrogram = random_speech[0]\n",
    "style_spec = random_speech[1]\n",
    "\n",
    "# Convert mel spectrogram to linear scale\n",
    "mel_to_linear = torchaudio.transforms.InverseMelScale(n_stft=800)\n",
    "linear_spectrogram = mel_to_linear(mel_spectrogram)\n",
    "\n",
    "# Convert linear spectrogram to waveform\n",
    "griffin_lim = torchaudio.transforms.GriffinLim()\n",
    "waveform = griffin_lim(linear_spectrogram)\n",
    "\n",
    "# Plot mel spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(mel_spectrogram.log2(), cmap='inferno', origin='lower', aspect='auto')\n",
    "\n",
    "# Set frequency axis ticks\n",
    "num_ticks = 10\n",
    "freq_bins = np.linspace(0, mel_spectrogram.shape[0], num_ticks)\n",
    "hz_ticks = torchaudio.transforms.MelScale().mel_to_hz(torch.tensor(freq_bins))\n",
    "plt.yticks(freq_bins, [\"{:.0f}\".format(hz) for hz in hz_ticks])\n",
    "\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.title('Mel Spectrogram')\n",
    "plt.show()\n",
    "\n",
    "# display(ipd.Audio(waveform.cpu().numpy(), rate=sample_rate))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d36eb4ffb1b509eac7c8ec69a0f00740540681bdf210a10d1e2607102c088e08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
