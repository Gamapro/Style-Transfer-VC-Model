{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import torch\n",
    "import torch.optim\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torchaudio import transforms\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoderLayer\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
    "from utils import WaveNet, AudioMELSpectogramDataset, CalcContentLoss, CalcStyleLoss, plot_mel_spectrogram, mel_to_wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialDecoder(nn.Module):\n",
    "    def __init__(self, decoder_list):\n",
    "        super(SequentialDecoder, self).__init__()\n",
    "        self.layers = nn.ModuleList(decoder_list)\n",
    "\n",
    "    def forward(self, input, memory):\n",
    "        for layer in self.layers:\n",
    "            input = layer(input, memory)\n",
    "        return input\n",
    "\n",
    "class StyleAutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, shared_channels, out_channels, kernel_size, dilation, nhead=8, dropout=0.2, dim_feedforward=1024):\n",
    "        super(StyleAutoEncoder, self).__init__()\n",
    "        self.sharedEncoder = WaveNet(in_channels, shared_channels, kernel_size, dilation, dropout)\n",
    "        self.speechEncoder = WaveNet(shared_channels, out_channels, kernel_size, dilation, dropout)\n",
    "        self.styleEncoder = WaveNet(shared_channels, out_channels, kernel_size, dilation, dropout)\n",
    "        encoder_layers_speech, encoder_layers_style, decoder_layers = [], [], []\n",
    "        for _ in range(4):\n",
    "            encoder_layer = self.EncoderLayer(out_channels, dim_feedforward, dropout)\n",
    "            encoder_layer_2 = self.EncoderLayer(out_channels, dim_feedforward, dropout)\n",
    "            decoder_layer = self.DecoderLayer(out_channels*2, dim_feedforward, dropout)\n",
    "            encoder_layers_speech.append(encoder_layer)\n",
    "            encoder_layers_style.append(encoder_layer_2)\n",
    "            decoder_layers.append(decoder_layer)\n",
    "        self.speech_encoder_attention = nn.Sequential(*encoder_layers_speech)\n",
    "        self.style_encoder_attention = nn.Sequential(*encoder_layers_style)\n",
    "        self.decoder_attention = SequentialDecoder(decoder_layers)\n",
    "        self.decoder = WaveNet(out_channels*2, in_channels, kernel_size, dilation, dropout)\n",
    "\n",
    "    def EncoderLayer(self, out_channels, dim_feedforward, dropout):\n",
    "        return TransformerEncoderLayer(\n",
    "            d_model=out_channels,\n",
    "            nhead=8,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation='relu'\n",
    "        )\n",
    "    \n",
    "    def DecoderLayer(self, out_channels, dim_feedforward, dropout):\n",
    "        return TransformerDecoderLayer(\n",
    "            d_model=out_channels,\n",
    "            nhead=8,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation='relu'\n",
    "        )\n",
    "\n",
    "    def forward(self, speech, style):\n",
    "        # print('speech', speech.shape)\n",
    "        # print('style', style.shape)\n",
    "\n",
    "        style = self.sharedEncoder(style)\n",
    "        speech = self.sharedEncoder(speech) \n",
    "\n",
    "        style = self.styleEncoder(style)\n",
    "        speech = self.speechEncoder(speech)\n",
    "        \n",
    "        style = style.permute(2, 0, 1)  # Reordenamos las dimensiones para que sea compatible con la capa de atenci√≥n\n",
    "        style_encoder = self.style_encoder_attention(style)\n",
    "        style = style.permute(1, 2, 0)\n",
    "        style_encoder = speech.permute(0, 1, 2)\n",
    "\n",
    "        speech = speech.permute(2, 0, 1)\n",
    "        speech_encoder = self.speech_encoder_attention(speech)\n",
    "        speech = speech.permute(1, 2, 0)\n",
    "        speech_encoder = speech.permute(0, 1, 2)\n",
    "        \n",
    "        # print('speech', speech.shape)\n",
    "        # print('style', style.shape)\n",
    "        out = torch.cat([speech, style], dim=1)\n",
    "        # print('speech_encoder', speech_encoder.shape)\n",
    "        # print('style_encoder', style_encoder.shape)\n",
    "        out_encoder = torch.cat([speech_encoder, style_encoder], dim=1)\n",
    "        \n",
    "        out = out.permute(2, 0, 1) \n",
    "        out_encoder = out.permute(0, 1, 2) \n",
    "\n",
    "        # print('out', out.shape)\n",
    "        # print('out_encoder', out_encoder.shape)\n",
    "\n",
    "        x = self.decoder_attention(out, out_encoder)\n",
    "        out = out.permute(1, 2, 0) \n",
    "\n",
    "        # print('x', x.shape)\n",
    "\n",
    "        out = self.decoder(out)\n",
    "        # x = x.unsqueeze(1)\n",
    "        # print('x', x.shape)\n",
    "\n",
    "        return out #.squeeze(0)\n",
    " \n",
    "class MelSpecVCAutoencoderModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, in_channels, shared_channels, out_channels, kernel_size, dilation, device, sample_rate, dropout=0.2):\n",
    "        super(MelSpecVCAutoencoderModule, self).__init__()\n",
    "        self.encoder_generator = StyleAutoEncoder(in_channels, shared_channels, out_channels, kernel_size, dilation, dropout)\n",
    "        self.todevice = device\n",
    "        self.speech_weight = 1\n",
    "        self.style_weight = 1000\n",
    "\n",
    "    def forward(self, speech, style):\n",
    "        y = self.encoder_generator(speech, style)\n",
    "        return y\n",
    "\n",
    "    def debug(self, loss_speech, loss_style, loss):\n",
    "        # print('loss_speech', loss_speech.item())\n",
    "        # print('loss_style', loss_style.item())\n",
    "        # print('shape', loss.shape)\n",
    "        # print('loss', loss.item())\n",
    "        pass\n",
    "\n",
    "    def apply_loss(self, batch):\n",
    "        speech, style = batch\n",
    "        if speech.ndim == 2: speech = speech.unsqueeze(1)\n",
    "        if style.ndim == 2: style = style.unsqueeze(1)\n",
    "        # print('speech', speech.shape)\n",
    "        # print('style', style.shape)\n",
    "        y = self(speech, style)\n",
    "        # print('y', y.shape)\n",
    "        loss_speech = CalcContentLoss(y, speech) * self.speech_weight\n",
    "        loss_style = CalcStyleLoss(y, style) * self.style_weight\n",
    "        loss = loss_speech + loss_style\n",
    "        self.debug(loss_speech, loss_style, loss)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.apply_loss(batch)\n",
    "        self.log('train_loss', loss.item())\n",
    "        return loss \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.apply_loss(batch)\n",
    "        self.log('val_loss', loss.item())\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_loss'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "sample_rate = 16000\n",
    "div_ratio = 0.8\n",
    "n_fft = 800\n",
    "audio_length = 3\n",
    "in_channels, shared_channels, out_channels, kernel_size, dilation = 128, 64, 32, 3, 12\n",
    "metadata = pd.read_csv('metadata.csv')\n",
    "dataset = AudioMELSpectogramDataset(metadata, device, sample_rate=sample_rate, audio_length=audio_length, n_fft=n_fft)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [int(len(dataset) * div_ratio), len(dataset) - int(len(dataset) * div_ratio)])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)\n",
    "model = MelSpecVCAutoencoderModule(in_channels, shared_channels, out_channels, kernel_size, dilation, device=device, sample_rate=sample_rate).to(device)\n",
    "model = model.to(torch.float64)\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath='./checkpoints',\n",
    "    filename='{epoch}-{val_loss:.2f}-{val_r2:.2f}',\n",
    "    save_top_k=1,\n",
    "    monitor='val_loss',\n",
    "    every_n_epochs =1,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    max_epochs=10,\n",
    "    benchmark=True,\n",
    "    # deterministic=True,\n",
    "    precision=16,\n",
    "    accumulate_grad_batches=6,\n",
    "    callbacks=[\n",
    "        checkpoint_callback,\n",
    "        pl.callbacks.LearningRateMonitor(logging_interval='step')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_speech = train_dataset[120]\n",
    "speech_spec = random_speech[0].to(device)\n",
    "style_spec = random_speech[1].to(device)\n",
    "display(ipd.Audio(mel_to_wav(speech_spec), rate=sample_rate))\n",
    "\n",
    "model.eval()\n",
    "speech_spec = speech_spec.unsqueeze(0)\n",
    "style_spec = style_spec.unsqueeze(0)\n",
    "y = model(speech_spec, style_spec)\n",
    "y = y.detach()\n",
    "display(ipd.Audio(mel_to_wav(y), rate=sample_rate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d36eb4ffb1b509eac7c8ec69a0f00740540681bdf210a10d1e2607102c088e08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
