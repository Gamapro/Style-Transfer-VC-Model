{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import torch\n",
    "import torch.optim\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pytorch_lightning import Trainer\n",
    "from torch import nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoderLayer\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
    "from utils import AudioDataset, WaveNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleAutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation, device, n_head=8, dropout=0.2):\n",
    "        super(StyleAutoEncoder, self).__init__()\n",
    "        self.styleEncoder = WaveNet(in_channels, out_channels, kernel_size, dilation, dropout)\n",
    "        self.speechEncoder = WaveNet(in_channels, out_channels, kernel_size, dilation, dropout)\n",
    "        self.style_encoder_attention = TransformerEncoderLayer(out_channels, nhead=n_head)\n",
    "        self.speech_encoder_attention = TransformerEncoderLayer(out_channels, nhead=n_head)\n",
    "        # self.decoder_attention = TransformerDecoderLayer(out_channels*2, nhead=n_head)\n",
    "        self.decoder = WaveNet(out_channels*2, in_channels, kernel_size, dilation, dropout)\n",
    "        # self.wav2vec = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        # self.tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "    def forward(self, speech, style):\n",
    "        if speech.ndim == 2: speech = speech.unsqueeze(1)\n",
    "        if style.ndim == 2: style = style.unsqueeze(1)\n",
    "\n",
    "        style = self.styleEncoder(style)\n",
    "        speech = self.speechEncoder(speech)\n",
    "        \n",
    "        style = style.permute(2, 0, 1)  # Reordenamos las dimensiones para que sea compatible con la capa de atención\n",
    "        style = self.style_encoder_attention(style)\n",
    "        style = style.permute(1, 2, 0)  # Restauramos el orden de las dimensiones\n",
    "        \n",
    "        speech = speech.permute(2, 0, 1)  # Reordenamos las dimensiones para que sea compatible con la capa de atención\n",
    "        speech = self.speech_encoder_attention(speech)\n",
    "        speech = speech.permute(1, 2, 0)  # Restauramos el orden de las dimensiones\n",
    "\n",
    "        x = torch.cat((style, speech), dim=1)\n",
    "        #x = x.permute(2, 0, 1)  # Reordenamos las dimensiones para que sea compatible con la capa de atención\n",
    "        # x = self.decoder_attention(x)\n",
    "        #x = x.permute(1, 2, 0)  # Restauramos el orden de las dimensiones\n",
    "\n",
    "        x = self.decoder(x)\n",
    "        # input_ids = self.tokenizer(x, return_tensors=\"pt\").input_values\n",
    "        #logits = self.wav2vec(input_ids).logits\n",
    "        # transcriptions = self.tokenizer.batch_decode(logits.logits, skip_special_tokens=True)\n",
    "        return x.squeeze(0) #, transcriptions\n",
    "\n",
    "\n",
    "class StyleAutoencoderModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation, device, dropout=0.2):\n",
    "        super(StyleAutoencoderModule, self).__init__()\n",
    "        self.model = StyleAutoEncoder(in_channels, out_channels, kernel_size, dilation, dropout)\n",
    "        self.loss_psnt = nn.MSELoss()\n",
    "        self.loss_id = nn.MSELoss()\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, speech, style):\n",
    "        return self.model(speech, style)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        speech, style = batch\n",
    "        y = self(speech, style)\n",
    "        loss_psnt = self.loss_psnt(y, speech)\n",
    "        loss_id = self.loss_id(y, style)\n",
    "        loss = loss_psnt + loss_id\n",
    "        return loss \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        speech, style = batch\n",
    "        y = self(speech, style)\n",
    "        loss_psnt = self.loss_psnt(y, speech)\n",
    "        loss_id = self.loss_id(y, style)\n",
    "        loss = loss_psnt + loss_id\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_loss'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "sample_rate = 16000\n",
    "metadata = pd.read_csv('metadata.csv')\n",
    "dataset = AudioDataset(metadata, device, sample_rate=sample_rate)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [int(len(dataset) * 0.8), len(dataset) - int(len(dataset) * 0.8)])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)\n",
    "model = StyleAutoEncoder(1, 8, 3, 12, device=device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit None Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    max_epochs=10,\n",
    "    benchmark=True,\n",
    "    # deterministic=True,\n",
    "    precision=16,\n",
    "    callbacks=[\n",
    "        pl.callbacks.ModelCheckpoint(monitor='val_loss', mode='min'),\n",
    "        pl.callbacks.LearningRateMonitor(logging_interval='step')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 7.63 GiB (GPU 0; 6.00 GiB total capacity; 3.46 MiB already allocated; 3.70 GiB free; 4.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\David Arcos\\Documents\\GitHub\\Style-Transfer-VC-Model\\style-encoder.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/David%20Arcos/Documents/GitHub/Style-Transfer-VC-Model/style-encoder.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/David%20Arcos/Documents/GitHub/Style-Transfer-VC-Model/style-encoder.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/David%20Arcos/Documents/GitHub/Style-Transfer-VC-Model/style-encoder.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     predicted_audio \u001b[39m=\u001b[39m model(random_audio\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m), style_audio\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/David%20Arcos/Documents/GitHub/Style-Transfer-VC-Model/style-encoder.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(random_audio\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/David%20Arcos/Documents/GitHub/Style-Transfer-VC-Model/style-encoder.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m display(ipd\u001b[39m.\u001b[39mAudio(random_audio\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy(), rate\u001b[39m=\u001b[39msample_rate))\n",
      "File \u001b[1;32mc:\\Users\\David Arcos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\David Arcos\\Documents\\GitHub\\Style-Transfer-VC-Model\\style-encoder.ipynb Cell 7\u001b[0m in \u001b[0;36mStyleAutoEncoder.forward\u001b[1;34m(self, speech, style)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/David%20Arcos/Documents/GitHub/Style-Transfer-VC-Model/style-encoder.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m speech \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspeechEncoder(speech)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/David%20Arcos/Documents/GitHub/Style-Transfer-VC-Model/style-encoder.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m style \u001b[39m=\u001b[39m style\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# Reordenamos las dimensiones para que sea compatible con la capa de atención\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/David%20Arcos/Documents/GitHub/Style-Transfer-VC-Model/style-encoder.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m style \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstyle_encoder_attention(style)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/David%20Arcos/Documents/GitHub/Style-Transfer-VC-Model/style-encoder.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m style \u001b[39m=\u001b[39m style\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)  \u001b[39m# Restauramos el orden de las dimensiones\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/David%20Arcos/Documents/GitHub/Style-Transfer-VC-Model/style-encoder.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m speech \u001b[39m=\u001b[39m speech\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# Reordenamos las dimensiones para que sea compatible con la capa de atención\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\David Arcos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\David Arcos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:538\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    536\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[0;32m    537\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 538\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[0;32m    539\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[0;32m    541\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\David Arcos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:546\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[0;32m    545\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 546\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[0;32m    547\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m    548\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[0;32m    549\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    550\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n",
      "File \u001b[1;32mc:\\Users\\David Arcos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\David Arcos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1167\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[0;32m   1156\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1157\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[0;32m   1158\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1164\u001b[0m         q_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight, k_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj_weight,\n\u001b[0;32m   1165\u001b[0m         v_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj_weight, average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights)\n\u001b[0;32m   1166\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1167\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[0;32m   1168\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[0;32m   1169\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[0;32m   1170\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[0;32m   1171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m   1172\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[0;32m   1173\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask, need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[0;32m   1174\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask, average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights)\n\u001b[0;32m   1175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[0;32m   1176\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32mc:\\Users\\David Arcos\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:5160\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[0;32m   5158\u001b[0m     attn_output_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbaddbmm(attn_mask, q_scaled, k\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m   5159\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 5160\u001b[0m     attn_output_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbmm(q_scaled, k\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[0;32m   5161\u001b[0m attn_output_weights \u001b[39m=\u001b[39m softmax(attn_output_weights, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m   5162\u001b[0m \u001b[39mif\u001b[39;00m dropout_p \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 7.63 GiB (GPU 0; 6.00 GiB total capacity; 3.46 MiB already allocated; 3.70 GiB free; 4.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "random_speech = train_dataset[0]\n",
    "random_audio = random_speech[0]\n",
    "style_audio = random_speech[1]\n",
    "\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "with torch.no_grad():\n",
    "    predicted_audio = model(random_audio.unsqueeze(0), style_audio.unsqueeze(0))\n",
    "\n",
    "print(random_audio.shape)\n",
    "display(ipd.Audio(random_audio.cpu().numpy(), rate=sample_rate))\n",
    "predicted_audio = predicted_audio.squeeze(0)\n",
    "print(predicted_audio.shape)\n",
    "display(ipd.Audio(predicted_audio.cpu().numpy(), rate=sample_rate))\n",
    "\n",
    "# plt.plot(random_audio.cpu().numpy())\n",
    "plt.plot(predicted_audio.cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VoxCelebTest/id10270/5r0dWxy17C8/00001.wav\n"
     ]
    }
   ],
   "source": [
    "random_audio = train_dataset[0]\n",
    "random_audio = random_audio.to(device)\n",
    "\n",
    "audio_path = metadata['path'][0]\n",
    "print(audio_path)\n",
    "\n",
    "# Cargar el modelo pre-entrenado y el tokenizer\n",
    "# wav2vec = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")#.to(device)\n",
    "# tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# input_ids = tokenizer(audio_path, return_tensors=\"pt\").input_values\n",
    "# logits = wav2vec(input_ids).logits\n",
    "# transcriptions = tokenizer.batch_decode(logits, skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d36eb4ffb1b509eac7c8ec69a0f00740540681bdf210a10d1e2607102c088e08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
