{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import torch\n",
    "import torch.optim\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoderLayer\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
    "from utils import AudioDataset, WaveNet, AudioMELSpectogramDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleAutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation, n_head=8, dropout=0.2):\n",
    "        super(StyleAutoEncoder, self).__init__()\n",
    "        self.sharedEncoder = WaveNet(in_channels, out_channels//2, kernel_size, dilation, dropout)\n",
    "        self.styleEncoder = WaveNet(out_channels//2, out_channels, kernel_size, dilation, dropout)\n",
    "        self.speechEncoder = WaveNet(out_channels//2, out_channels, kernel_size, dilation, dropout)\n",
    "        self.style_encoder_attention = TransformerEncoderLayer(out_channels, nhead=2)\n",
    "        self.speech_encoder_attention = TransformerEncoderLayer(out_channels, nhead=2)\n",
    "        self.decoder_attention = TransformerEncoderLayer(out_channels*2, nhead=2)\n",
    "        self.decoder = WaveNet(out_channels*2, in_channels, kernel_size, dilation, dropout)\n",
    "        # self.wav2vec = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        # self.tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "    def forward(self, speech, style):\n",
    "\n",
    "        style = self.sharedEncoder(style)\n",
    "        speech = self.sharedEncoder(speech) \n",
    "\n",
    "        style = self.styleEncoder(style)\n",
    "        speech = self.speechEncoder(speech)\n",
    "        \n",
    "        style = style.permute(2, 0, 1)  # Reordenamos las dimensiones para que sea compatible con la capa de atención\n",
    "        style = self.style_encoder_attention(style)\n",
    "        style = style.permute(1, 2, 0)  # Restauramos el orden de las dimensiones\n",
    "        \n",
    "        speech = speech.permute(2, 0, 1)  # Reordenamos las dimensiones para que sea compatible con la capa de atención\n",
    "        speech = self.speech_encoder_attention(speech)\n",
    "        speech = speech.permute(1, 2, 0)  # Restauramos el orden de las dimensiones\n",
    "\n",
    "        x = torch.cat((style, speech), dim=1)\n",
    "        x = x.permute(2, 0, 1)  # Reordenamos las dimensiones para que sea compatible con la capa de atención\n",
    "        x = self.decoder_attention(x)\n",
    "        x = x.permute(1, 2, 0)  # Restauramos el orden de las dimensiones\n",
    "\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x.squeeze(0) #, transcriptions\n",
    "    \n",
    "class WaveNetDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation, sample_rate, dropout=0.2):\n",
    "        super(WaveNetDiscriminator, self).__init__()\n",
    "        self.wavenet = WaveNet(in_channels, in_channels, kernel_size, dilation, dropout)\n",
    "        self.linear1 = nn.Linear(in_channels * sample_rate, out_channels)\n",
    "        self.linear2 = nn.Linear(out_channels, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.wavenet(x)\n",
    "        output = output.view(x.shape[0], -1)\n",
    "        output = self.linear1(output)\n",
    "        output = self.linear2(output)\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "class StyleAutoencoderModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation, device, sample_rate, dropout=0.2):\n",
    "        super(StyleAutoencoderModule, self).__init__()\n",
    "        self.encoder_generator = StyleAutoEncoder(in_channels, out_channels, kernel_size, dilation, dropout)\n",
    "        self.encoder_discriminator = WaveNetDiscriminator(in_channels, out_channels, kernel_size, dilation, sample_rate, dropout)\n",
    "        self.decoder_generator = StyleAutoEncoder(in_channels, out_channels, kernel_size, dilation, dropout)\n",
    "        self.decoder_discriminator = WaveNetDiscriminator(in_channels, out_channels, kernel_size, dilation, sample_rate, dropout)\n",
    "        self.loss_speech_loss_fn = nn.MSELoss()\n",
    "        self.loss_style_loss_fn = nn.MSELoss()\n",
    "        self.disc_loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.todevice = device\n",
    "\n",
    "    def forward(self, speech, style):\n",
    "        y = self.encoder_generator(speech, style)\n",
    "        y1 = self.decoder_generator(y, style)\n",
    "        enc_disc_out = self.encoder_discriminator(y)\n",
    "        dec_disc_out = self.decoder_discriminator(y1)\n",
    "        labels = torch.zeros(y.shape[0], 1, device=self.device)\n",
    "        return y, y1, enc_disc_out, dec_disc_out, labels\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        speech, style = batch\n",
    "        if speech.ndim == 2: speech = speech.unsqueeze(1)\n",
    "        if style.ndim == 2: style = style.unsqueeze(1)\n",
    "        y, y1, enc_disc_out, dec_disc_out, labels = self(speech, style)\n",
    "        loss_speech = self.loss_speech_loss_fn(y, speech)\n",
    "        loss_style = self.loss_style_loss_fn(y1, style)\n",
    "        loss_disc_speech = self.disc_loss_fn(enc_disc_out,labels) + self.disc_loss_fn(dec_disc_out,labels)\n",
    "        loss = loss_speech + loss_style + loss_disc_speech\n",
    "        self.log('train_loss', loss.item())\n",
    "        return loss \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        speech, style = batch\n",
    "        if speech.ndim == 2: speech = speech.unsqueeze(1)\n",
    "        if style.ndim == 2: style = style.unsqueeze(1)\n",
    "        y, y1, enc_disc_out, dec_disc_out, labels = self(speech, style)\n",
    "        loss_speech = self.loss_speech_loss_fn(y, speech)\n",
    "        loss_style = self.loss_style_loss_fn(y1, style)\n",
    "        loss_disc_speech = self.disc_loss_fn(enc_disc_out,labels) + self.disc_loss_fn(dec_disc_out,labels)\n",
    "        loss = loss_speech + loss_style + loss_disc_speech\n",
    "        self.log('val_loss', loss.item())\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-2)\n",
    "        # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_loss'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium') #  | 'high'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "sample_rate = 16000\n",
    "metadata = pd.read_csv('metadata.csv')\n",
    "dataset = AudioDataset(metadata, device, sample_rate=sample_rate)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [int(len(dataset) * 0.7), len(dataset) - int(len(dataset) * 0.7)])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)\n",
    "model = StyleAutoencoderModule(1, 64, 3, 12, device=device, sample_rate=sample_rate)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath='./checkpoints',\n",
    "    filename='{epoch}-{val_loss:.2f}-{val_r2:.2f}',\n",
    "    save_top_k=1,\n",
    "    monitor='val_loss',\n",
    "    every_n_epochs =1,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    max_epochs=10,\n",
    "    benchmark=True,\n",
    "    # deterministic=True,\n",
    "    precision=16,\n",
    "    accumulate_grad_batches=6,\n",
    "    callbacks=[\n",
    "        checkpoint_callback,\n",
    "        pl.callbacks.LearningRateMonitor(logging_interval='step')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_speech = train_dataset[0]\n",
    "random_audio = random_speech[0]\n",
    "style_audio = random_speech[1]\n",
    "\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "random_audio = random_audio.unsqueeze(0)\n",
    "style_audio = style_audio.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predicted_audio = model.encoder_generator(random_audio.unsqueeze(0), style_audio.unsqueeze(0))\n",
    "\n",
    "print(random_audio.shape, style_audio.shape, predicted_audio.shape)\n",
    "\n",
    "display(ipd.Audio(random_audio.cpu().numpy(), rate=sample_rate))\n",
    "display(ipd.Audio(style_audio.cpu().numpy(), rate=sample_rate))\n",
    "predicted_audio = predicted_audio.squeeze(0)\n",
    "display(ipd.Audio(predicted_audio.cpu().numpy(), rate=sample_rate))\n",
    "\n",
    "# print(random_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(random_audio.cpu().numpy())\n",
    "plt.plot(style_audio.cpu().numpy())\n",
    "plt.plot(predicted_audio.cpu().numpy())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d36eb4ffb1b509eac7c8ec69a0f00740540681bdf210a10d1e2607102c088e08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
